{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.8: Racetrack (programming) Consider driving a race car around a\n",
    "# turn like those shown in Figure 5.6. You want to go as fast as possible, but not so fast\n",
    "# as to run off the track. In our simplified racetrack, the car is at one of a discrete set\n",
    "# of grid positions, the cells in the diagram. The velocity is also discrete, a number of\n",
    "# grid cells moved horizontally and vertically per time step. The actions are increments\n",
    "# to the velocity components. Each may be changed by +1, −1, or 0 in one step, for a\n",
    "# total of nine actions. Both velocity components are restricted to be nonnegative and\n",
    "# less than 5, and they cannot both be zero except at the starting line. Each episode\n",
    "# begins in one of the randomly selected start states with both velocity components\n",
    "# zero and ends when the car crosses the finish line. The rewards are −1 for each step\n",
    "# until the car crosses the finish line. If the car hits the track boundary, it is moved\n",
    "# back to a random position on the starting line, both velocity components are reduced\n",
    "# to zero, and the episode continues. Before updating the car’s location at each time\n",
    "# step, check to see if the projected path of the car intersects the track boundary. If\n",
    "# it intersects the finish line, the episode ends; if it intersects anywhere else, the car is\n",
    "# considered to have hit the track boundary and is sent back to the starting line. To\n",
    "# make the task more challenging, with probability 0.1 at each time step the velocity\n",
    "# increments are both zero, independently of the intended increments. Apply a Monte\n",
    "# Carlo control method to this task to compute the optimal policy from each starting\n",
    "# state. Exhibit several trajectories following the optimal policy (but turn the noise\n",
    "# off for these trajectories). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#notes on track\n",
    "#setting it up as dictionary where the coordinates are the keys and the values are what part of the track\n",
    "    #track, off-track, start line, boundary\n",
    "#for intersections, I'm tracing a hypothetical line on every movement from the start to the finish\n",
    "    #I follow the path of that line in increments of 1\n",
    "    #at every increment, check to see if in a reward or off-track square\n",
    "    #if so then end the episode/restart the game, if not then continue along the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting up the track\n",
    "        \n",
    "#track: consists of blocks featured plus an additional boundary boxes along the edges\n",
    "#track 1\n",
    "#17 blocks wide plus an additional left boundary for 18\n",
    "#32 blocks high with an additional top boundary\n",
    "#going cartesian coordinate stile where 0,0 is the lower left\n",
    "#labeling the coordinates as: 0 for track, 1 for start line, 2 for off-track, 3 for reward line\n",
    "track_1 = {}\n",
    "for i in range(18):\n",
    "    for j in range(32):\n",
    "        if i == 0:\n",
    "            track_1[(i,j)] = 2\n",
    "        elif j == 31:\n",
    "            track_1[(i,j)] = 2\n",
    "        elif i == 1 and ( j < 18 or j > 26):\n",
    "            track_1[(i,j)] = 2\n",
    "        elif i == 2 and ( j < 10 or j > 27):\n",
    "            track_1[(i,j)] = 2\n",
    "        elif i == 3 and ( j < 3 or j > 29):\n",
    "            track_1[(i,j)] = 2\n",
    "        elif i > 9 and j < 25:\n",
    "            track_1[(i,j)] = 2\n",
    "        elif i > 10 and j < 26:\n",
    "            track_1[(i,j)] = 2\n",
    "        else:\n",
    "            track_1[(i,j)] = 0\n",
    "\n",
    "#starting line\n",
    "starting_line = []\n",
    "for i in range(4,10):\n",
    "    starting_line.append((i,0))\n",
    "    track_1[(i,0)] = 1\n",
    "    \n",
    "#finish line\n",
    "#finish_line = []\n",
    "for j in range(26,32):\n",
    "    #finish_line.append((17,j))\n",
    "    track_1[(17,j)] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convenience functions for dealing with track\n",
    "\n",
    "#get random starting coordinates\n",
    "def random_start(start):\n",
    "    return start[np.random.randint(0,len(start))]\n",
    "\n",
    "#get type of track\n",
    "def get_track_type(track,x,y):\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    return track[(x,y)]\n",
    "\n",
    "#get the track position where the car finished\n",
    "# def get_finish_position(track,x,y,x_vel,y_vel):\n",
    "#     return track[(x+x_vel,y+y_vel)]\n",
    "\n",
    "#get spaces passed on movement\n",
    "#not completely accurate as I'm moving over 1 coordinate at a time so could clip into off track\n",
    "def boundary_track_passed(track,x_start,y_start,x_vel,y_vel):\n",
    "    #move_list = [] #passing all the coordinates moved through to here\n",
    "    \n",
    "    #move_length = np.sqrt(x_vel**2 + y_vel**2)\n",
    "    slope = y_vel/x_vel\n",
    "    for i in range(1,x_vel+1):\n",
    "        track_type = get_track_type(track,x_start+1,y_vel+slope*i)\n",
    "        if track_type >= 2:\n",
    "            return track_type\n",
    "        \n",
    "    return 0 #ie still on the board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement a MC algorithm\n",
    "    #can do it like I solved cartpole with the Q table, take the current reward, update the state with the current reward and\n",
    "        #the best possible future reward (iterated over possible actions)\n",
    "    #many options:\n",
    "        #on or off policy\n",
    "        #exploring starts or not\n",
    "        #first value or not\n",
    "    #going to go with Monte Carlo ES from section 5.3\n",
    "\n",
    "    \n",
    "#convenience functions\n",
    "def get_best_reward_and_action(q_table,pos,vel):\n",
    "    temp_list = get_action_list(vel[0],vel[1])\n",
    "    best_action = temp_list[0]\n",
    "    best_reward = q_table[(pos,vel,temp_list[0])]\n",
    "    \n",
    "    for i in temp_list:\n",
    "        temp_reward = q_table[(pos,vel,i)]\n",
    "        if temp_reward > best_reward:\n",
    "            best_reward = temp_reward\n",
    "            best_action = i\n",
    "            \n",
    "    return best_reward, best_action\n",
    "        \n",
    "\n",
    "#can't increment below 0, can't have both be 0,0, neither can go above 5\n",
    "def get_action_list(x_vel,y_vel):\n",
    "    pi_a = []\n",
    "    if x_vel + y_vel >= 1:\n",
    "        pi_a.append(0,0)\n",
    "        \n",
    "        if x_vel < 5:\n",
    "            pi_a.append(1,0)\n",
    "        if y_vel < 5:\n",
    "            pi_a.append(0,1)\n",
    "        if x_vel < 5 and y_vel < 5:\n",
    "            pi_a.append(1,1)\n",
    "        \n",
    "        if x_vel > 0 and y_vel > 0:\n",
    "            pi_a.append(-1,-1)\n",
    "        if x_vel > 0:\n",
    "            pi_a.append(-1,0)\n",
    "        if y_vel > 0:\n",
    "            pi_a.append(0,-1)\n",
    "    else \n",
    "        pi_a.append(1,0)\n",
    "        pi_a.append(0,1)\n",
    "        pi_a.append(1,1)\n",
    "\n",
    "    return pi_a\n",
    "    \n",
    "#initialize Q(s,a) and pi(s)\n",
    "Q = {} #value of state\n",
    "pi_s = {} #best action found so far in that state\n",
    "returns = {} #dictionary of empty lists\n",
    "\n",
    "for key in track_1:\n",
    "    track_type = track_1[key]\n",
    "    if track_type >= 2:\n",
    "        continue\n",
    "    else if track_type == 1:\n",
    "        pi_s[(key,(0,0))] = (1,1)\n",
    "        #starting track\n",
    "        x_vel = 0\n",
    "        y_vel = 0\n",
    "        pi_possible = get_action_list(x_vel,y_vel)\n",
    "        for a in pi_possible:   \n",
    "            Q[(key,(x_vel,y_vel),a)] = 0\n",
    "            returns[(key,(x_vel,y_vel),a)] = []\n",
    "        \n",
    "    else:\n",
    "        for x_vel in range(0,6):\n",
    "            for y_vel in range(0,6):\n",
    "                if i + j = 0:\n",
    "                    continue\n",
    "                pi_s[(key,(x_vel,y_vel))] = (0,0)\n",
    "                else:\n",
    "                    pi_possible = get_action_list(x_vel,y_vel)\n",
    "                    \n",
    "                for a in pi_possible:\n",
    "                    Q[(key,(x_vel,y_vel),a)] = 0\n",
    "                    returns[(key,(x_vel,y_vel),a)] = []\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unsure of what explicitly is meant by G. can do the current reward -1 for each step\n",
    "#need to follow a policy so that all things equally likely\n",
    "#could go random for a bit and see who it ends up but might take too long with calculations\n",
    "episodes = 1000\n",
    "explore_max = 0.95\n",
    "explore_min = 0.5\n",
    "steps = 0\n",
    "gamma = 1\n",
    "\n",
    "for i in range(episodes):\n",
    "    start_pos = random_start(start_line)\n",
    "    current_pos = start_pos\n",
    "    x_vel = 0\n",
    "    y_vel = 0\n",
    "    #go on the track until finish line or reward\n",
    "    on_track = True\n",
    "    while on_track:\n",
    "        steps += 1\n",
    "        explore_p = explore_max - explore_max * steps/episodes + explore_min\n",
    "        \n",
    "        action_list = get_action_list(x_vel,y_vel)\n",
    "        if np.random.rand() <= explore_p:\n",
    "            #do a random action\n",
    "            action = action_list[np.random.randint(0,len(action_list))]\n",
    "        else:\n",
    "            action = get_best_action()\n",
    "        \n",
    "        boundary_check = boundary_track_passed(track_1,current_pos[0],current_pos[1],action[0],action[1])\n",
    "        if boundary_check == 3:\n",
    "            #ends\n",
    "            G = -1 + gamma*0\n",
    "            rewards[(current_pos,(x_vel,y_vel),action)].append(G)\n",
    "            break\n",
    "        elif boundary_check == 2:\n",
    "            #grab a new starting position\n",
    "            next_pos = random_start(start_line)\n",
    "        else:\n",
    "            #still on the track\n",
    "            next_pos = (current_pos[0] + action[0],current_pos[1]+action[1])\n",
    " \n",
    "        #Getting returns G. uncertain how they want me to calculate G. ie how to use next state and discount factor\n",
    "        next_reward, _ = get_best_reward_and_action(Q,next_pos,(x_vel,y_vel))\n",
    "        G = -1 + gamma*next_reward \n",
    "        returns[(current_pos,(x_vel,y_vel),action)].append(G)\n",
    "        \n",
    "        #set Q(s,a)\n",
    "        #Q is the average of the returns\n",
    "        Q[(current_pos,(x_vel,y_vel),action)] = np.mean(returns[(current_pos,(x_vel,y_vel),action)])\n",
    "\n",
    "        #set pi(s)\n",
    "        _, best_action = get_best_reward_and_action(Q,next_pos,(x_vel,y_vel))\n",
    "        pi[(current_pos,(x_vel,y_vel))] = best_action\n",
    "        \n",
    "        #setting the position and velocity for the next step\n",
    "        current_pos = next_pos\n",
    "        if boundary_check == 2:\n",
    "            x_vel = 0\n",
    "            y_vel = 0\n",
    "        else:\n",
    "            x_vel += action[0]\n",
    "            y_vel += action[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print out the Q table\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf35]",
   "language": "python",
   "name": "conda-env-tf35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
